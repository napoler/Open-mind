---
title: 潜在语义分析
author: TerryChan
date: 2021-03-21
category: nlp
layout: post
---

# 潜在语义分析（latent semantic analysis）

编辑日期: Mar 21, 2021 2:48 PM

**潜在语义分析**（Latent Semantic Analysis），是[语义学](https://zh.wikipedia.org/wiki/%E8%AF%AD%E4%B9%89%E5%AD%A6)的一个新的分支。传统的语义学通常研究字、词的含义以及词与词之间的关系，如同义，近义，反义等等。潜在语义分析探讨的是隐藏在字词背后的某种关系，这种关系不是以词典上的定义为基础，而是以字词的使用环境作为最基本的参考。这种思想来自于心理语言学家。他们认为，世界上数以百计的语言都应该有一种共同的简单的机制，使得任何人只要是在某种特定的语言环境下长大都能掌握那种语言。在这种思想的指导下，人们找到了一种简单的数学模型，这种模型的输入是由任何一种语言书写的文献构成的文库，输出是该语言的字、词的一种数学表达（向量）。字、词之间的关系乃至任何文章片断之间的含义的比较就由这种向量之间的运算产生。

**潜在语义学**的观念也被应用在[资讯检索](https://zh.wikipedia.org/wiki/%E8%B3%87%E8%A8%8A%E6%AA%A2%E7%B4%A2)上，所以有时潜在语义学也被称为**[隐含语义索引](https://zh.wikipedia.org/wiki/%E9%9A%B1%E5%90%AB%E8%AA%9E%E7%BE%A9%E7%B4%A2%E5%BC%95)**（Latent Semantic Indexing，LSI）。

- [ ]  

****目录****

- [1概述] 
- [2应用](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6#%E6%87%89%E7%94%A8)
- [3降维](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6#%E9%99%8D%E7%B6%AD)
- [4实现](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6#%E5%AE%9E%E7%8E%B0)
- [5另见](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6#%E5%8F%A6%E8%A7%81)
- [6参考文献](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE)

## 概述 
隐含语义索引是一种以[向量空间模型](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F%E7%A9%BA%E9%96%93%E6%A8%A1%E5%9E%8B)为基底的[资讯检索](https://zh.wikipedia.org/wiki/%E8%B3%87%E8%A8%8A%E6%AA%A2%E7%B4%A2)技术，常以字词－文件矩阵表示字词与文件之间的关联；而其多以行代表字词〈term〉，列代表文件〈document〉。

而在矩阵中每个元素的权重值以[TF-IDF](https://zh.wikipedia.org/wiki/TF-IDF)计算后得到。该字词在某个文件中的重要性，与该字词在该文件中出现的次数成正比，与其在所有文件中出现的次数成反比。

而这个字词－文件矩阵本身也代表着一个标准的语义模组，因为数学矩阵的格式并不是经常的被使用，所以不会太特别的注明其为一个矩阵的型态。

## 应用 

关于隐含语义索引的其他应用：

1. 比对文件之间的概念（[资料挖掘](https://zh.wikipedia.org/wiki/%E8%B3%87%E6%96%99%E6%8C%96%E6%8E%98)、[文件分类](https://zh.wikipedia.org/w/index.php?title=%E6%96%87%E4%BB%B6%E5%88%86%E9%A1%9E&action=edit&redlink=1)）
2. 分析后比对不同语言文章的相似度（[跨语检索](https://zh.wikipedia.org/wiki/%E8%B7%A8%E8%AA%9E%E6%AA%A2%E7%B4%A2)）
3. 找出字词间的关联（[同义词](https://zh.wikipedia.org/wiki/%E5%90%8C%E7%BE%A9%E8%A9%9E)或是[多义性](https://zh.wikipedia.org/w/index.php?title=%E5%A4%9A%E7%BE%A9%E6%80%A7&action=edit&redlink=1)）
4. 给定一个查询词汇，转换到概念空间，且找出符合的文件（[资讯检索](https://zh.wikipedia.org/wiki/%E8%B3%87%E8%A8%8A%E6%AA%A2%E7%B4%A2)）

同义词和多义性是自然语言处理中最基本的问题。

同义现象是指不同的词语表示相同的的意思。因此，在搜索引擎不会返回一个文档，即使它与该查询是相关的，只是由于它不包含查询关键词中的词语。例如，查询“doctors”（医生）时，不会返回包含“physicians”（内科医生）的文档，即使它们的意思相同。

多义现象是指一个词语含有多种意思。因此，查询的结果可能返回许多不相关的文档，只是由于它们包含了查询关键词中的词语。例如，植物学家和计算机专家在查询同一个关键词“树”的时候，它们希望得到的结果是完全不同的。

## 降维 

当生成了词汇－文档[矩阵](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3)后，LSA提供了一种对它的低维近似（可以通过对字词—文档矩阵的奇异值分解（SVD）来实现）。做这种近似有以下几种原因：

1. 对原始的词汇-文档矩阵进行计算时，计算量太大。而低维矩阵提供了一种近似（尽量少但却不可避免地有一些信息丢失）。
2. 原始的矩阵一般包含噪声（垃圾信息）。在这种意义上，近似的低维矩阵是一种去噪矩阵（比原始矩阵更好）。
3. 原始的词汇－文档矩阵过度地[稀疏](https://zh.wikipedia.org/wiki/%E7%A8%80%E7%96%8F%E7%9F%A9%E9%99%A3)。它罗列了每篇文档中的实际出现的词汇，而由于同义词的存在，我们关心的是所有地与文档有关系的词汇集合，这个集合一般要比实际出现的词汇集合要大得多。

## 实现 

奇异值分解（svd）是一种典型的使用大型矩阵的运算方法，会占用较大的存储空间，可以通过一种类似神经网络的计算方法来大大降低这一计算对内存的占用。现在已经有一种快速的，占用内存较低的，计算大型矩阵的svd算法，见应用文献【3】.可以使用MATLAB和Python来实现这一算法。

## 另见 

- [反向索引](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E7%B4%A2%E5%BC%95)
- [复合词组处理](https://zh.wikipedia.org/w/index.php?title=%E5%A4%8D%E5%90%88%E8%AF%8D%E7%BB%84%E5%A4%84%E7%90%86&action=edit&redlink=1)（Compound term processing）

## 参考文献 

- Handbook of Latent Semantic Analysis, Edited by Thomas K Landauer, Danielle S. McNamara, Simon Dennis and Walter Kintsch, Lawrence Erlbaum Associates, Inc., 2007.
- [科罗拉多大学的潜在语义学网上工具](http://lsa.colorado.edu/)
- Matthew Brand (2006). "Fast Low-Rank Modifications of the Thin Singular Value Decomposition" (PDF). Linear Algebra and Its Applications 415: 20–30. doi:10.1016/j.laa.2005.07.021

维基百科地址 https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%AD%A6
