---
title: bert
author: TerryChan
date: 2021-03-21
category: nlp
layout: post
---


BERT 模型是 Google 在 2018 年提出的一种 NLP 模型，成为最近几年 NLP 领域最具有突破性的一项技术。在 11 个 NLP 领域的任务上都刷新了以往的记录，例如GLUE，SquAD1.1，MultiNLI 等。

##  前言
Google 在论文《BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding》中提出了 BERT 模型，BERT 模型主要利用了 Transformer 的 Encoder 结构，采用的是最原始的 Transformer，对 Transformer 不熟悉的童鞋可以参考一下之前的文章《Transformer 模型详解》或者 Jay Alammar 的博客:The Illustrated Transformer。总的来说 BERT 具有以下的特点：

结构：采用了 Transformer 的 Encoder 结构，但是模型结构比 Transformer 要深。Transformer Encoder 包含 6 个 Encoder block，BERT-base 模型包含 12 个 Encoder block，BERT-large 包含 24 个 Encoder block。

## 训练：
训练主要分为两个阶段：预训练阶段和 Fine-tuning 阶段。预训练阶段与 Word2Vec，ELMo 等类似，是在大型数据集上根据一些预训练任务训练得到。Fine-tuning 阶段是后续用于一些下游任务的时候进行微调，例如文本分类，词性标注，问答系统等，BERT 无需调整结构就可以在不同的任务上进行微调。

预训练任务1：BERT 的第一个预训练任务是 Masked LM，在句子中随机遮盖一部分单词，然后同时利用上下文的信息预测遮盖的单词，这样可以更好地根据全文理解单词的意思。Masked LM 是 BERT 的重点，和 biLSTM 预测方法是有区别的，后续会讲到。

预训练任务2：BERT 的第二个预训练任务是 Next Sentence Prediction (NSP)，下一句预测任务，这个任务主要是让模型能够更好地理解句子间的关系。

## 可用资源

- [Google Bert](https://github.com/google-research/bert)
- [transformers Bert](https://huggingface.co/transformers/model_doc/bert.html)
了解NLP的读者应该对Hugging Face这个名字非常熟悉了。他们制作了Transformers（GitHub超1.5万星）、neuralcoref、pytorch-pretrained-BigGAN等非常流行的模型。通过它们的接口可以轻松调用bert模型。
- [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
Whole Word Masking (wwm)，暂翻译为全词Mask或整词Mask，是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。
- [知识蒸馏工具TextBrewer](https://github.com/airaria/TextBrewer)



